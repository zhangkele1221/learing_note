{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDSJUuDsE5rQOz3Wi/JRT3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangkele1221/learing_note/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# python test sample"
      ],
      "metadata": {
        "id": "yRDURLErh1WR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello world ! welcome to colab \")"
      ],
      "metadata": {
        "id": "y4Bct-Ash6KT",
        "outputId": "cc07a637-3e9b-4ad0-9704-c73a10a84b34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world ! welcome to colab \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = 10\n",
        "b = 100\n",
        "c = a+b\n",
        "print(\"result\",c)"
      ],
      "metadata": {
        "id": "qA_xkoEfieBr",
        "outputId": "b6614592-da3d-4463-a8a0-d7f8a5513ea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# c or c++ demo  runing"
      ],
      "metadata": {
        "id": "DDuF8PPekC6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.cc\n",
        "#include<iostream>\n",
        "\n",
        "int main(){\n",
        "  std::cout<<\"hello world\"<<std::endl;\n",
        "  return 1;\n",
        "}\n"
      ],
      "metadata": {
        "id": "n2uLHjGEkClQ",
        "outputId": "1f5480fe-b82c-4ea6-c5e9-22079cbb6f38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test.cc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -g -Wall -o test test.cc"
      ],
      "metadata": {
        "id": "stfyJ4fwknEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./test"
      ],
      "metadata": {
        "id": "b_uqHhkbkt2t",
        "outputId": "60851083-abc8-4070-fc4a-d633cc50bedb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hello.c\n",
        "#include<stdio.h>\n",
        "int main(){\n",
        "  printf(\"hello world\");\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "EdVtxip9k7JX",
        "outputId": "4945c00f-837c-4039-fb9e-e8cdb0b9f3b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcc  -g -Wall -o hello hello.c"
      ],
      "metadata": {
        "id": "9MZXezlwlNJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello"
      ],
      "metadata": {
        "id": "KXBe4m5TlS2t",
        "outputId": "43d15182-ac51-4d51-b91e-dd844d2a3dd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# cuda leaing and runing\n"
      ],
      "metadata": {
        "id": "U0z0BPTAl5v2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 检查 nvcc 编译器"
      ],
      "metadata": {
        "id": "hMfqjY0Yo9hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!which nvcc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbOlTTZCpVmZ",
        "outputId": "da249326-2789-4d3c-e632-9856fa18ca72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda/bin/nvcc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 查看nvcc 版本"
      ],
      "metadata": {
        "id": "Q6XCRrX6pFYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT2R0riPpc64",
        "outputId": "c6e4ae0f-60ea-40ad-9880-eecbb520343b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 检查环境是否与gpu链接上"
      ],
      "metadata": {
        "id": "lEy3GdzopgmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITpxCHk8p6Pj",
        "outputId": "bf6d7e00-ea34-4e34-d2c6-dcee2df7a813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep  4 15:30:15 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 写cuda 代码"
      ],
      "metadata": {
        "id": "v2Xpi77Nqik_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  hello_cuda.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int gpu_devices_num;\n",
        "    cudaError_t err = cudaGetDeviceCount(&gpu_devices_num);\n",
        "\n",
        "    if (err != cudaSuccess)\n",
        "    {\n",
        "        std::cout << \"Error Message : \" << cudaGetErrorString(err) << std::endl;\n",
        "    }\n",
        "\n",
        "    std::cout << \"  GPU num : \" << gpu_devices_num << std::endl;\n",
        "\n",
        "    for (int i = 0; i < gpu_devices_num; ++i)\n",
        "    {\n",
        "        cudaDeviceProp prop;\n",
        "        cudaGetDeviceProperties(&prop, i);\n",
        "\n",
        "        std::cout << \"  Device Number: \" << i << std::endl;\n",
        "        std::cout << \"  Device name: \" << prop.name << std::endl;\n",
        "        std::cout << \"  Memory Clock Rate (KHz): \" << prop.memoryClockRate << std::endl;\n",
        "        std::cout << \"  Memory Bus Width (bits): \" << prop.memoryBusWidth << std::endl;\n",
        "        std::cout << \"  Peak Memory Bandwidth (GB/s): \" << 2.0 * prop.memoryClockRate * (prop.memoryBusWidth / 8) / 1.0e6 << std::endl;\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGQN75xsqmZC",
        "outputId": "8acbd193-51e9-46f6-9940-f7de270ff6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hello_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 编译 cuda代码"
      ],
      "metadata": {
        "id": "WJeCvnSVrItu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc   hello_cuda.cu -o hello_cuda"
      ],
      "metadata": {
        "id": "2fsSu1jerM_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f3a4562-69f0-4d62-8b0c-245c57831cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Khello_cuda.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 运行cuda 程序"
      ],
      "metadata": {
        "id": "2V5lcKCBriuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubP4m3LZrmjo",
        "outputId": "c82c7844-e926-4107-c887-9ee993673787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  GPU num : 1\n",
            "  Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoYdkN2gxPWB",
        "outputId": "b63dbeab-db4d-479c-a7fb-0e3290196985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cmake version 3.30.2\n",
            "\n",
            "CMake suite maintained and supported by Kitware (kitware.com/cmake).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# other"
      ],
      "metadata": {
        "id": "CpN7_jJYxn2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2qNIFnixsC0",
        "outputId": "3e9c1b69-e2ac-42a2-dd5c-534a7b19b614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"\" > test.sh"
      ],
      "metadata": {
        "id": "tDKuhjpc3bH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Nv9geX_T6Jwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/test.sh"
      ],
      "metadata": {
        "id": "9IWxck-33_p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/test.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA5JN02p4GqN",
        "outputId": "c225e675-e27c-44c5-ec8e-bad8ac4782a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " https://gitee.com/llteacher/cuda-learn   还不错的基cuda基础代码库\n",
        " https://gitee.com/cui-rongpei/cuda-learn 这个也OK"
      ],
      "metadata": {
        "id": "QYbYpS0WmK7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  cuda1.cu\n",
        "#include <cstdio>\n",
        "using namespace std;\n",
        "\n",
        "//这是一个CUDA核函数 通过__global__关键字定义 这意味着这个函数将在GPU上执行 并且由主机代码调用。\n",
        "__global__ void hell_from__gpu()\n",
        "{\n",
        "    // 核函数不支持 c++ 的 iostream。\n",
        "    // 输出流的缓存顺序。\n",
        "    // printf(\"gpu: hello world! \");\n",
        "\n",
        "    /*\n",
        "    bid = blockIdx.z * (gridDim.x * gridDim.y) + blockIdx.y * (gridDim.x) + blockIdx.x\n",
        "    */\n",
        "\n",
        "    const int bx = blockIdx.x; //获取当前块的X轴索引\n",
        "    const int by = blockIdx.y; //获取当前块的Y轴索引\n",
        "    const int bz = blockIdx.z; //获取当前块的Z轴索引\n",
        "\n",
        "    const int tx = threadIdx.x; //获取当前线程的X轴索引\n",
        "    const int ty = threadIdx.y; //获取当前线程的Y轴索引\n",
        "    const int tz = threadIdx.z; //获取当前线程的Z轴索引\n",
        "\n",
        "    // 打印块和线程的3D索引\n",
        "    printf(\"gpu: hello world! block(%d, %d, %d) -- thread(%d, %d, %d)\\n\",\n",
        "            bx, by, bz, tx, ty, tz);\n",
        "\n",
        "    //gpu: hello world! block(0, 0, 0) -- thread(0, 0, 0)\n",
        "    //gpu: hello world! block(0, 0, 0) -- thread(1, 0, 0)\n",
        "    //gpu: hello world! block(0, 0, 0) -- thread(0, 1, 0)\n",
        "    //gpu: hello world! block(0, 0, 0) -- thread(1, 1, 0)\n",
        "\n",
        "    //gpu: hello world! block(0, 0, 0) -- thread(0, 2, 0)\n",
        "    //gpu: hello world! block(0, 0, 0) -- thread(1, 2, 0)\n",
        "    //gpu: hello world! block(0, 0, 0) -- thread(0, 3, 0)\n",
        "    //gpu: hello world! block(0, 0, 0) -- thread(1, 3, 0)\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    printf(\"nvcc: hello world!\\n\");\n",
        "    // 定义2x4大小的线程块尺寸\n",
        "    const dim3 block_size(2, 4); //定义线程块的大小。这里定义了一个2x4的块, 表示每个块包含8个线程。\n",
        "\n",
        "    // Launch the kernel\n",
        "    //表示启动配置,第一个参数1表示网格(grid)中仅有一个块(block),第二个参数block_size表示每个块中有2x4个线程。\n",
        "    //这个网格只有一个线程块 所以看后面输出的 z 坐标都是0 理解其中的意\n",
        "    hell_from__gpu<<<1, block_size>>>();\n",
        "\n",
        "    cudaDeviceSynchronize(); // 同步主机和设备，否则无法输出字符串。\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z8B0qR_7Kejr",
        "outputId": "08b4f5fe-d538-4ddf-ed86-0e131823ef05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc   cuda1.cu -o cuda1"
      ],
      "metadata": {
        "id": "3ZSPvpDFK1PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./cuda1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6LUzQUgK7Rq",
        "outputId": "b8d1800d-9bba-4436-ffee-7bdba6a91a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: hello world!\n",
            "gpu: hello world! block(0, 0, 0) -- thread(0, 0, 0)\n",
            "gpu: hello world! block(0, 0, 0) -- thread(1, 0, 0)\n",
            "gpu: hello world! block(0, 0, 0) -- thread(0, 1, 0)\n",
            "gpu: hello world! block(0, 0, 0) -- thread(1, 1, 0)\n",
            "gpu: hello world! block(0, 0, 0) -- thread(0, 2, 0)\n",
            "gpu: hello world! block(0, 0, 0) -- thread(1, 2, 0)\n",
            "gpu: hello world! block(0, 0, 0) -- thread(0, 3, 0)\n",
            "gpu: hello world! block(0, 0, 0) -- thread(1, 3, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile  cuda2.cu\n",
        "#include <cstdio>\n",
        "#include <iostream>\n",
        "using namespace std;\n",
        "\n",
        "// 两个向量加法kernel，grid和block均为一维\n",
        "__global__ void add(float* x, float * y, float* z, int n)\n",
        "{\n",
        "    // 获取全局索引\n",
        "    int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    // 步长\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "    for (int i = index; i < n; i += stride)\n",
        "    {\n",
        "        z[i] = x[i] + y[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int N = 1 << 20;\n",
        "    int nBytes = N * sizeof(float);\n",
        "    // 申请host内存\n",
        "    float *x, *y, *z;\n",
        "    x = (float*)malloc(nBytes);\n",
        "    y = (float*)malloc(nBytes);\n",
        "    z = (float*)malloc(nBytes);\n",
        "\n",
        "    // 初始化数据\n",
        "    for (int i = 0; i < N; ++i)\n",
        "    {\n",
        "        x[i] = 10.0;\n",
        "        y[i] = 20.0;\n",
        "    }\n",
        "\n",
        "    // 申请device内存\n",
        "    float *d_x, *d_y, *d_z;\n",
        "    cudaMalloc((void**)&d_x, nBytes);\n",
        "    cudaMalloc((void**)&d_y, nBytes);\n",
        "    cudaMalloc((void**)&d_z, nBytes);\n",
        "\n",
        "    // 将host数据拷贝到device\n",
        "    cudaMemcpy((void*)d_x, (void*)x, nBytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy((void*)d_y, (void*)y, nBytes, cudaMemcpyHostToDevice);\n",
        "    // 定义kernel的执行配置\n",
        "    dim3 blockSize(256);\n",
        "    dim3 gridSize((N + blockSize.x - 1) / blockSize.x);\n",
        "    // 执行kernel\n",
        "    add << < gridSize, blockSize >> >(d_x, d_y, d_z, N);\n",
        "\n",
        "    // 将device得到的结果拷贝到host\n",
        "    cudaMemcpy((void*)z, (void*)d_z, nBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // 检查执行结果\n",
        "    float maxError = 0.0;\n",
        "    for (int i = 0; i < N; i++)\n",
        "        maxError = fmax(maxError, fabs(z[i] - 30.0));\n",
        "    std::cout << \"最大误差: \" << maxError << std::endl;\n",
        "\n",
        "    // 释放device内存\n",
        "    cudaFree(d_x);\n",
        "    cudaFree(d_y);\n",
        "    cudaFree(d_z);\n",
        "    // 释放host内存\n",
        "    free(x);\n",
        "    free(y);\n",
        "    free(z);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNq8Q1fE8yfW",
        "outputId": "36d8b8eb-e49b-4882-9fb6-3313887d5452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc   cuda2.cu -o cuda2"
      ],
      "metadata": {
        "id": "FbaosyIy83-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./cuda2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMFX5R8M9Nbw",
        "outputId": "934ec21f-f5a7-450d-c8f2-6e7a96fe3aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "最大误差: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile  hello4.cu\n",
        "#include <cstdio>\n",
        "using namespace std;\n",
        "\n",
        "\n",
        "__global__ void hell_from__gpu()\n",
        "{\n",
        "    // 核函数不支持 c++ 的 iostream。\n",
        "\n",
        "    // 输出流的缓存顺序。\n",
        "    // printf(\"gpu: hello world! \");\n",
        "\n",
        "    const int bx = blockIdx.x;\n",
        "    const int tx = threadIdx.x;\n",
        "\n",
        "    printf(\"gpu: hello world! block(%d) -- thread(%d)\\n\", bx, tx);\n",
        "}\n",
        "// 输出如下内容  当前线程  所属的 一个网格中线程块指标     blockIdx.x  取值范围是  [0-gridDim.x-1)  gridDim.x值等于 gridSize的数值\n",
        "//             当前线程  所属的 一个线程块中线程指标     threadIdx.x 取值范围是  [0-blockDim.x-1) blockDim.x值等于 blockSize的数值\n",
        "/*\n",
        "nvcc: hello world!\n",
        "//第一个线程块的 4个线程\n",
        "gpu: hello world! block(0) -- thread(0)\n",
        "gpu: hello world! block(0) -- thread(1)\n",
        "gpu: hello world! block(0) -- thread(2)\n",
        "gpu: hello world! block(0) -- thread(3)\n",
        "//第二个线程块的 4个线程\n",
        "gpu: hello world! block(1) -- thread(0)\n",
        "gpu: hello world! block(1) -- thread(1)\n",
        "gpu: hello world! block(1) -- thread(2)\n",
        "gpu: hello world! block(1) -- thread(3)\n",
        "*/\n",
        "// 上面两个线程块有时候回颠倒执行顺序 这个反应了一个重要特征就是每一个线程块都是相互独立的\n",
        "\n",
        "int main()\n",
        "{\n",
        "    printf(\"nvcc: hello world!\\n\");\n",
        "\n",
        "    hell_from__gpu<<<2, 4>>>();//gridSize   blockSize\n",
        "    cudaDeviceSynchronize(); // 同步主机和设备，否则无法输出字符串。\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1887dfbp-iT",
        "outputId": "26dc5179-6338-4b47-96ce-b231457df6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hello4.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc  hello4.cu -o hello4"
      ],
      "metadata": {
        "id": "BVNX6vbOr_iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./hello4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cHcqlDesHv6",
        "outputId": "5e564c60-64fc-4843-cd43-7a0c29bca433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: hello world!\n",
            "gpu: hello world! block(0) -- thread(0)\n",
            "gpu: hello world! block(0) -- thread(1)\n",
            "gpu: hello world! block(0) -- thread(2)\n",
            "gpu: hello world! block(0) -- thread(3)\n",
            "gpu: hello world! block(1) -- thread(0)\n",
            "gpu: hello world! block(1) -- thread(1)\n",
            "gpu: hello world! block(1) -- thread(2)\n",
            "gpu: hello world! block(1) -- thread(3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 推广至多维网格\n",
        "\n",
        "blockIdx 和 threadIdx 是类型uint3的变量，该类型是一个结构体，具有x，y，z 三个成员，所以 blockIdx.x只是其中一个成员，剩下两个是 blockIdx.y blockIdx.z 类似的threadIdx.x threadIdx.y threadIdx.z\n",
        "\n",
        "所以我们 前面的一次类推就有了\n",
        "  blockIdx.x 的范围是 [0-gridDim.x-1)\n",
        "  blockIdx.y 的范围是 [0-gridDim.y-1)\n",
        "  blockIdx.z 的范围是 [0-gridDim.z-1)\n",
        "\n",
        "  threadIdx.x 的范围是 [0-blockDim.x-1)\n",
        "  threadIdx.y 的范围是 [0-blockDim.y-1)\n",
        "  threadIdx.z 的范围是 [0-blockDim.z-1)\n",
        "\n",
        "hello_from_gpu<<<2, 4>>> 表示的是\n",
        "\n",
        "\n",
        "//线程在线程块上的 ID\n",
        "tid = threadIdx.z * (blockDim.x * blockDim.y)// 当前线程块上前面的所有线程数\n",
        "    + threadIdx.y * (blockDim.x)             // 当前线程块上当前面上前面行的所有线程数\n",
        "    + threadIdx.x                            // 当前线程块上当前面上当前行的线程数\n",
        "\n",
        "\n",
        "//线程块在网格上的 ID\n",
        "bid = blockIdx.z * (gridDim.x * gridDim.y)\n",
        "    + blockIdx.y * (gridDim.x)\n",
        "    + blockIdx.x\n",
        "\n",
        "\n",
        "开普勒架构到图灵架构的 GPU，网格大小在 x, y, z 方向的最大允许值为 （2^31 - 1, 2^16 - 1, 2^16 -1）；\n",
        "线程块大小在 x, y, z 方向的最大允许值为 （1024， 1024， 64），同时要求一个线程块最多有 1024 个线程。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zqKC8QeZud82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q colabcode"
      ],
      "metadata": {
        "id": "IT3PZkyDJzor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " <<< Block 数, 每个Block中的 Thread 数>>>\n",
        "\n",
        "启动核函数时，核函数代码由我们自行配置的 Block 中的每个 Thread 执行。\n",
        "因此，如果假设已定义一个名为 someKernel 的核函数，则GPU线程可以配置为下列情况：\n",
        "\n",
        "someKernel<<<1, 1>>() 在GPU中为该核函数分配1个具有1个线程的线程块，核函数中的代码将只运行1次；\n",
        "\n",
        "someKernel<<<1, 10>>() 在GPU中为该核函数分配1个具有10个线程的线程块，核函数中的代码将运行10次；\n",
        "\n",
        "someKernel<<<10, 1>>() 在GPU中为该核函数分配10个具有1个线程的线程块，核函数中的代码将运行10次；\n",
        "\n",
        "someKernel<<<10, 10>>() 在GPU中为该核函数分配10个具有10个线程的线程块，核函数中的代码将运行100次；\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G1_ivXscBOK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  hello5.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "// 核函数\n",
        "__global__ void printSuccessForCorrectExecutionConfiguration() {\n",
        "  // 当执行到第255个线程块的第1023个线程时，才输出\n",
        "  if(threadIdx.x == 1023 && blockIdx.x == 255) {\n",
        "    printf(\"Success!\\n\"); // 输出 Success！\n",
        "    printf(\"threadIdx.x: %d\\n\", threadIdx.x); // 输出线程ID\n",
        "    printf(\"blockIdx.x: %d\\n\", blockIdx.x); // 输出线程块ID\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // 配置该核函数由256个含有1024个线程的线程块中执行\n",
        "  printSuccessForCorrectExecutionConfiguration<<<256, 1024>>>();\n",
        "  cudaDeviceSynchronize(); // 同步\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkRkcZ8mPDE9",
        "outputId": "2f7db4ce-8ba5-46ba-e651-a8989610dcaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello5.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc  hello5.cu -o hello5"
      ],
      "metadata": {
        "id": "p2nPE9koPKSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./hello5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w59tV4fbPOkA",
        "outputId": "5f2015ce-78da-4c54-d740-68e4ea6c8995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "threadIdx.x: 1023\n",
            "blockIdx.x: 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "管理不同块之间的线程\n",
        "之前提到过，一个线程块可以包含多个线程，那么我们就可以调整线程块的大小以实现更多类型的并行化。线程块包含的线程具有数量限制：确切地说是 1024 个（即每个块中的线程数量 <= 1024）。通常为了增加加速应用程序中的并行量，我们需要利用多个线程块，并在它们之间进行协调。\n",
        "\n",
        "CUDA 核函数中，记录了每个块中线程数的变量是 blockDim.x（一个线程块中包含的线程数量，每个块中包含的线程数都是一样的）。通过将此变量与 blockIdx.x 和 threadIdx.x 变量结合使用，并借助表达式 threadIdx.x + blockIdx.x * blockDim.x 计算线程ID。该表达式可以用C++中访问二维数组的索引计算来类比看待，以增强理解。\n",
        "\n",
        "以下是详细示例：\n",
        "\n",
        "配置参数 <<<10, 10>>> 将启动共计拥有 100 个线程的网格，该网格又分为由 10 个线程组成的 10 个线程块（即一个线程块中含有10个线程，blockDim.x=10）。这时候，就可以利用表达式 threadIdx.x + blockIdx.x * blockDim.x 来计算某个线程的唯一索引（0 至 99 之间）了。\n",
        "\n",
        "如果线程块 blockIdx.x 索引为 0，则 blockIdx.x * blockDim.x 为 0。以 0 为起始索引加上可能的 threadIdx.x 值（0 至 9），便可在网格中找到索引为 0 至 9 的线程。\n",
        "\n",
        "如果线程块 blockIdx.x 索引为 1，则 blockIdx.x * blockDim.x 为 10。以 10 为起始索引加上可能的 threadIdx.x 值（0 至 9），便可在网格中找到索引为 10 至 19 的线程。\n",
        "\n",
        "如果线程块 blockIdx.x 索引为 5，则 blockIdx.x * blockDim.x 为 50。以 50 为起始索引加上可能的 threadIdx.x 值（0 至 9），便可在网格中找到索引为 50 至 59 的线程。\n",
        "\n",
        "如果线程块 blockIdx.x 索引为 9，则 blockIdx.x * blockDim.x 为 90。以 90 为起始索引加上可能的 threadIdx.x 值（0 至 9），便可在网格中找到索引为 90 至 99 的线程。\n",
        "\n",
        "                            \n"
      ],
      "metadata": {
        "id": "-MxQ3nvuP9qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile  hello6.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void loop()\n",
        "{\n",
        "  // 在Grid中遍历所有thread\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  printf(\"%d\\n\", i);\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  /*\n",
        "   * 配置参数还可以试试其他的，例如：\n",
        "   * <<<5, 2>>>\n",
        "   * <<<10, 1>>>\n",
        "   */\n",
        "  loop<<<2, 5>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88nggI4WKz_m",
        "outputId": "e761759a-b8b6-4883-990a-2d2be2479470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello6.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc  hello6.cu -o hello6"
      ],
      "metadata": {
        "id": "siqyXtFIK8xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGC7-a6zK_t_",
        "outputId": "7a7afc25-3706-4717-b088-7a1f423e554f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "分配可同时被GPU和CPU访问的内存\n",
        "CUDA 的最新版本（版本 6 和更高版本）可以便捷地分配和释放既可用于 Host 也可被 Device 访问的内存。\n",
        "\n",
        "在 Host（CPU）中，我们一般适用malloc 和 free 来分配和释放内存，但这样分配的内存无法直接被Device（GPU）访问，所以在这里我们用cudaMallocManaged 和 cudaFree 两个函数来分配和释放同时可被 Host 和 Device 访问的内存。如下例所示：\n",
        "\n",
        "```\n",
        "// CPU\n",
        "int N = 10;\n",
        "size_t size = N * sizeof(int);\n",
        "int *a;\n",
        "\n",
        "a = (int *)malloc(size); // 分配CPU内存\n",
        "free(a); // 释放CPU内存\n",
        "\n",
        "\n",
        "// GPU\n",
        "int N = 10;\n",
        "size_t size = N * sizeof(int);\n",
        "int *a;\n",
        "\n",
        "cudaMallocManaged(&a, size);// 为a分配CPU和GPU内存\n",
        "cudaFree(a); // 释放GPU内存\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "实际上，cudaMallocManaged在统一内存中创建了一个托管内存池（CPU上有，GPU上也有），内存池中已分配的空间可以通过相同的指针直接被CPU和GPU访问，底层系统在统一的内存空间中自动地在设备和主机间进行传输。数据传输对应用来说是透明的，大大简化了代码。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nD9Df24vLr6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  hello7.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// 初始化数组\n",
        "void init(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i) {\n",
        "    a[i] = i;\n",
        "  }\n",
        "}\n",
        "\n",
        "// CUDA 核函数，所有元素乘2\n",
        "__global__ void doubleElements(int *a, int N) {\n",
        "  int i;\n",
        "  i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (i < N) {\n",
        "    a[i] *= 2;\n",
        "  }\n",
        "}\n",
        "\n",
        "// 检查数组内所有元素的值是否均为复数\n",
        "bool checkElementsAreDoubled(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i) {\n",
        "    if (a[i] != i*2) return false;\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int N = 1000;\n",
        "  int *a;\n",
        "\n",
        "  size_t size = N * sizeof(int);\n",
        "  cudaMallocManaged(&a, size); // 为a分配CPU和GPU空间\n",
        "\n",
        "  init(a, N); // 为数组a赋值\n",
        "  size_t threads_per_block = 256; // 定义每个block的thread数量\n",
        "\n",
        "  // 这个公式的目的是确保所有元素都能被处理\n",
        "  // 这个操作确保在N不能被完全整除时，计算结果会向上取整。这是通过增加threads_per_block - 1来实现的。\n",
        "  //    / threads_per_block: 这个操作计算需要多少个块来覆盖所有元素\n",
        "  size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block; // 定义block的数量\n",
        "\n",
        "  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N); // 执行核函数\n",
        "  cudaDeviceSynchronize(); // 同步\n",
        "\n",
        "  bool areDoubled = checkElementsAreDoubled(a, N); // 检查元素是否为复数\n",
        "  printf(\"All elements were doubled? %s\\n\", areDoubled ? \"TRUE\" : \"FALSE\");\n",
        "\n",
        "  cudaFree(a); // 释放由cudaMallocManaged\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mp_HMYPMXV0",
        "outputId": "04b5aeb6-926a-4169-f4a6-561617e9d6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello7.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc  hello7.cu -o hello7"
      ],
      "metadata": {
        "id": "rY4Auru8UoHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FU2xt7KUsng",
        "outputId": "f88db8e7-1f39-463f-8755-0bf725e70fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All elements were doubled? TRUE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "网格大小与实际并行工作量不匹配\n",
        "\n",
        "网格大于工作量\n",
        "\n",
        "鉴于 GPU 的硬件特性，线程块中的线程数最好配置为 32 的倍数。但是在实际工作中，很可能会出现这样的情况，我们手动配置参数所创建的线程数无法匹配为实现并行循环所需的线程数，比如实际上需要执行1230次循环，但是你却配置了2048个线程。\n",
        "\n",
        "我们不可能每次配置参数的时候都手动去算一遍最佳配置，更何况并不是所有的数都是 32 的倍数。不过这个问题现在已经可以通过以下三个步骤轻松地解决：\n",
        "\n",
        "首先，设置配置参数，使线程总数超过实际工作所需的线程数。\n",
        "然后，在向核函数传递参数时传递一个用于表示要处理的数据集总大小或完成工作所需的总线程数 N。\n",
        "最后，计算网格内的线程索引后（使用 threadIdx + blockIdx*blockDim），判断该索引是否超过 N，只在不超过的情况下执行与核函数相关的工作。\n",
        "以下是一种可选的配置方式，适用于 工作总量 N 和线程块中的线程数已知的情况。如此一来，便可确保网格中至少始终能执行 N 次任务，且最多只浪费 1 个线程块的线程数量：\n",
        "\n",
        "```\n",
        "// 假设N是已知的\n",
        "int N = 100000;\n",
        "\n",
        "// 把每个block中的thread数设为256\n",
        "size_t threads_per_block = 256;\n",
        "\n",
        "// 根据N和thread数量配置Block数量\n",
        "size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "some_kernel<<<number_of_blocks, threads_per_block>>>(N);\n",
        "\n",
        "\n",
        "```\n",
        "由于上述执行配置致使网格中的线程数超过 N，因此需要注意 some_kernel 定义中的内容，以确保 some_kernel 在由其中一个额外的（大于N的）线程执行时不会尝试访问超出范围的数据元素，也就是要加个判断：\n",
        "\n",
        "```\n",
        "__global__ some_kernel(int N) {\n",
        "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "  if (idx < N) { // 保证线程ID小于元素数量N\n",
        "    // 并行代码\n",
        "  }\n",
        "\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "mK_6UeoIVGm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  hello8.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void initializeElementsTo(int initialValue, int *a, int N) {\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if (i < N) {\n",
        "    a[i] = initialValue;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int N = 1000;\n",
        "\n",
        "  int *a;\n",
        "  size_t size = N * sizeof(int);\n",
        "  cudaMallocManaged(&a, size);\n",
        "\n",
        "  size_t threads_per_block = 256;\n",
        "  // 这是惯用的CUDA语法\n",
        "  // 为 number_of_blocks 分配一个值，以确保线程数至少与指针 a 中可供访问的元素数同样多。\n",
        "  size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "  int initialValue = 6; // 初始化的值\n",
        "  initializeElementsTo<<<number_of_blocks, threads_per_block>>>(initialValue, a, N);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // 检查元素值是否被初始化\n",
        "  for (int i = 0; i < N; ++i) {\n",
        "    if(a[i] != initialValue) {\n",
        "      printf(\"FAILURE: target value: %d\\t a[%d]: %d\\n\", initialValue, i, a[i]);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "  printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  cudaFree(a);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISWJjhoJW4zv",
        "outputId": "462b8320-4fd8-46be-d2fb-5ee35d46a263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hello8.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc  hello8.cu -o hello8"
      ],
      "metadata": {
        "id": "cG6tmWRnW-l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIGZQ0xyXATG",
        "outputId": "911e82b7-9db8-4380-c866-0233066772ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "网格小于工作量\n",
        "\n",
        "\n",
        "有时，工作量比网格大，或者出于某种原因，一个网格中的线程数量可能会小于实际工作量的大小。请思考一下包含 1000 个元素的数组和包含 250 个线程的网格（此处使用极小的规模以便于说明）。此网格中的每个线程将需使用 4 次。\n",
        "\n",
        "如要实现此操作，一种常用方法便是在核函数中使用跨网格循环。\n",
        "\n",
        "在跨网格循环中，每个线程将在网格内使用 threadIdx + blockIdx*blockDim 计算自身唯一的索引，并对数组内该索引的元素执行相应运算，然后用网格中的线程数加上自身索引值，并重复此操作，直至超出数组范围。\n",
        "\n",
        "例如，对于包含 500 个元素的数组 a 和包含 250 个线程的网格，网格中索引为 20 的线程将执行如下操作：\n",
        "\n",
        "对 a[20] 执行相应运算；\n",
        "\n",
        "将线程索引增加 250，使网格的大小达到 270\n",
        "对a[270] 执行相应运算；\n",
        "\n",
        "将线程索引增加 250，使网格的大小达到 520\n",
        "由于 520 现已超出数组范围，因此线程将停止工作。\n",
        "\n",
        "CUDA 提供一个记录了网格中线程块数的变量：gridDim.x\n",
        "然后可以利用它来计算网格中的总线程数，即网格中的线程块数乘以每个线程块中的线程数：gridDim.x * blockDim.x。\n",
        "\n",
        "**现在来看看以下核函数中网格跨度循环的示例：**\n",
        "\n",
        "```\n",
        "__global void kernel(int *a, int N)\n",
        "{\n",
        "  int indexWithinTheGrid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int gridStride = gridDim.x * blockDim.x; // grid 的一个跨步\n",
        "\n",
        "  for (int i = indexWithinTheGrid; i < N; i += gridStride) {\n",
        "    // 对 a[i] 的操作;\n",
        "  }\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "xKTglT7RXZG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  hello9.cu\n",
        "\n",
        "// 使用了跨网格循环来处理比网格更大的数组：\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// 初始化数组a\n",
        "void init(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i) {\n",
        "    a[i] = i;\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void doubleElements(int *a, int N) {\n",
        "\n",
        "  // 使用grid-stride循环，这样每个线程可以处理数组中的多个元素。\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = gridDim.x * blockDim.x; // grid 的一个跨步\n",
        "\n",
        "  for (int i = idx; i < N; i += stride) {\n",
        "    a[i] *= 2;\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "// 检查数组内所有元素的值是否均为复数\n",
        "bool checkElementsAreDoubled(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i) {\n",
        "    if (a[i] != i*2) return false;\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int N = 10000;\n",
        "  int *a;\n",
        "  size_t size = N * sizeof(int);\n",
        "  cudaMallocManaged(&a, size);\n",
        "\n",
        "  init(a, N); // 初始化数组a\n",
        "\n",
        "  size_t threads_per_block = 256; // 每个block的thread数量\n",
        "  size_t number_of_blocks = 32; // block数量\n",
        "\n",
        "  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  bool areDoubled = checkElementsAreDoubled(a, N);\n",
        "\n",
        "  // 检查数组内所有元素的值是否均为复数\n",
        "  printf(\"All elements were doubled? %s\\n\", areDoubled ? \"TRUE\" : \"FALSE\");\n",
        "\n",
        "  cudaFree(a);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_x-CGiSaFGE",
        "outputId": "eda7f996-9bb5-4949-9877-0fd3de5d62ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello9.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc  hello9.cu -o hello9"
      ],
      "metadata": {
        "id": "UTVY8UH3aG7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaqDWGbRaGkw",
        "outputId": "3abd0541-44f1-436e-90a9-276028cdfb7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All elements were doubled? TRUE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "错误处理\n",
        "CUDA 函数发生错误时会返回一个类型为 cudaError_t 的变量，该变量可用于检查调用函数时是否发生错误。以下是对调用 cudaMallocManaged 函数执行错误处理的示例：\n",
        "\n",
        "```\n",
        "cudaError_t err;\n",
        "err = cudaMallocManaged(&a, N)          // 假设a和N已经被定义\n",
        "\n",
        "if (err != cudaSuccess) { // `cudaSuccess` 是一个 CUDA 变量.\n",
        "  printf(\"Error: %s\\n\", cudaGetErrorString(err)); // `cudaGetErrorString` 是一个 CUDA 函数.\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "但是，核函数并不会返回类型为 cudaError_t 的值（因为核函数的返回值为void）。为检查执行核函数时是否发生错误（例如配置错误），CUDA 提供了 cudaGetLastError 函数，可以用于检查核函数执行期间发生的错误。\n",
        "\n",
        "```\n",
        "// 这段程序中的核函数会出一个CUDA错误，但是核函数本身无法捕获该错误\n",
        "someKernel<<<1, -1>>>();  // 线程数不能为-1\n",
        "\n",
        "cudaError_t err;\n",
        "err = cudaGetLastError(); // `cudaGetLastError` 会捕获上面代码中的最近的一个错误\n",
        "if (err != cudaSuccess) {\n",
        "  printf(\"Error: %s\\n\", cudaGetErrorString(err));\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "另一个要注意的点是，为了捕捉在异步核函数执行期间发生的错误，一定要检查后续同步 CPU 与 GPU 时 API 调用所返回的状态（例如 cudaDeviceSynchronize）；如果之前执行的某一个核函数失败了，则将会返回错误。\n",
        "\n",
        "```\n",
        "#include <stdio.h>\n",
        "\n",
        "// 初始化数组a\n",
        "void init(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i) {\n",
        "    a[i] = i;\n",
        "  }\n",
        "}\n",
        "\n",
        "// CUDA 核函数 数组元素值乘2\n",
        "__global__ void doubleElements(int *a, int N) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = gridDim.x * blockDim.x;\n",
        "\n",
        "  // for (int i = idx; i < N; i += stride) {\n",
        "  // 这里出现一个数值越界错误\n",
        "  for (int i = idx; i < N + stride; i += stride) {\n",
        "    a[i] *= 2;\n",
        "  }\n",
        "}\n",
        "\n",
        "// 检查数组元素是否均为复数\n",
        "bool checkElementsAreDoubled(int *a, int N) {\n",
        "  int i;\n",
        "  for (i = 0; i < N; ++i) {\n",
        "    if (a[i] != i*2) return false;\n",
        "  }\n",
        "  return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int N = 10000;\n",
        "  int *a;\n",
        "\n",
        "  size_t size = N * sizeof(int);\n",
        "  cudaMallocManaged(&a, size);\n",
        "  init(a, N);\n",
        "\n",
        "  cudaError_t syncErr, asyncErr; // 定义错误处理变量\n",
        "  \n",
        "  // size_t threads_per_block = 1024;\n",
        "  // 线程数大于1024（前面说过每个block的线程数不能超过1024）\n",
        "  size_t threads_per_block = 2048;\n",
        "  size_t number_of_blocks = 32;\n",
        "  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N); // 执行核函数\n",
        "\n",
        "  syncErr = cudaGetLastError(); // 捕获核函数执行期间发生的错误\n",
        "  asyncErr = cudaDeviceSynchronize(); // 同步，并捕获同步期间发生的错误\n",
        "\n",
        "  // 输出错误 说明：两个错误需分别设置（即每次运行时只保留一个错误）\n",
        "  if (syncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(syncErr));\n",
        "  if (asyncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr));\n",
        "\n",
        "  bool areDoubled = checkElementsAreDoubled(a, N); // 验证数组元素值是否均为复数\n",
        "  printf(\"All elements were doubled? %s\\n\", areDoubled ? \"TRUE\" : \"FALSE\");\n",
        "\n",
        "  cudaFree(a);\n",
        "}\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        " 定制一个 CUDA 错误处理宏\n",
        "创建一个包装 CUDA 函数调用的宏对于检查错误十分有用。以下是一个宏示例，我们可以在其他的 CUDA 代码中随时使用：\n",
        "\n",
        "```\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// CUDA 错误处理宏\n",
        "inline cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "  return result;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // checkCuda 宏可以返回 CUDA 函数返回的错误类型`cudaError_t`的值\n",
        "  checkCuda( cudaDeviceSynchronize() )\n",
        "}\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "8tZaMeZqIA3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  hello10.cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// CUDA 错误处理宏\n",
        "inline cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "  return result;\n",
        "}\n",
        "\n",
        "// 初始化数组 a\n",
        "void initWith(float num, float *a, int N) {\n",
        "  for(int i = 0; i < N; ++i) {\n",
        "    a[i] = num;\n",
        "  }\n",
        "}\n",
        "\n",
        "// 向量加法核函数\n",
        "__global__ void addVectorsInto(float *result, float *a, float *b, int N) {\n",
        "  int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  for(int i = index; i < N; i += stride) {\n",
        "    result[i] = a[i] + b[i]; // 元素a[i] + 元素 b[i]\n",
        "  }\n",
        "}\n",
        "\n",
        "// 检查 CUDA 向量加分是否计算正确\n",
        "void checkElementsAre(float target, float *array, int N) {\n",
        "  for(int i = 0; i < N; i++) {\n",
        "    if(array[i] != target) {\n",
        "      printf(\"FAIL: array[%d] - %0.0f does not equal %0.0f\\n\", i, array[i], target);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "  printf(\"SUCCESS! All values added correctly.\\n\");\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  const int N = 10;\n",
        "  size_t size = N * sizeof(float);\n",
        "\n",
        "  float *a;\n",
        "  float *b;\n",
        "  float *c;\n",
        "\n",
        "  // 分配内存，且检查执行期间发生的错误\n",
        "  checkCuda( cudaMallocManaged(&a, size) );\n",
        "  checkCuda( cudaMallocManaged(&b, size) );\n",
        "  checkCuda( cudaMallocManaged(&c, size) );\n",
        "\n",
        "  initWith(3, a, N); // 将数组a中所有的元素初始化为3\n",
        "  initWith(4, b, N); // 将数组b中所有的元素初始化为4\n",
        "  initWith(0, c, N); // 将数组c中所有的元素初始化为0，数组c是结果向量\n",
        "\n",
        "  // 配置参数\n",
        "  size_t threadsPerBlock = 256;\n",
        "  size_t numberOfBlocks = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "  addVectorsInto<<<numberOfBlocks, threadsPerBlock>>>(c, a, b, N); // 执行核函数\n",
        "\n",
        "  checkCuda( cudaGetLastError() ); // 检查核函数执行期间发生的错误\n",
        "  checkCuda( cudaDeviceSynchronize() ); // 同步，且检查执行期间发生的错误\n",
        "\n",
        "  checkElementsAre(7, c, N);  // 检查向量加的结果是否正确\n",
        "\n",
        "  // 释放内存，且检查执行期间发生的错误\n",
        "  checkCuda( cudaFree(a) );\n",
        "  checkCuda( cudaFree(b) );\n",
        "  checkCuda( cudaFree(c) );\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSL3CJGIJwMY",
        "outputId": "6f3b5455-59c6-42cf-b188-ba71b9b329df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello10.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc  hello10.cu -o hello10"
      ],
      "metadata": {
        "id": "NTo4VegPJ35z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkog2qMsJ93M",
        "outputId": "c1576e4c-ee05-4507-b38c-92724410c279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS! All values added correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  hello11.cu\n",
        "\n",
        "// 矩阵乘法\n",
        "\n",
        "#include <stdio.h>\n",
        "#define N  64\n",
        "\n",
        "// GPU 矩阵乘法\n",
        "__global__ void matrixMulGPU( int * a, int * b, int * c ) {\n",
        "  int val = 0;\n",
        "\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < N && col < N) {\n",
        "\n",
        "    for ( int k = 0; k < N; ++k ){\n",
        "      val += a[row * N + k] * b[k * N + col];\n",
        "    }\n",
        "\n",
        "    c[row * N + col] = val;\n",
        "\n",
        "  }\n",
        "}\n",
        "\n",
        "// CPU矩阵乘法\n",
        "void matrixMulCPU( int * a, int * b, int * c ) {\n",
        "  int val = 0;\n",
        "\n",
        "  for( int row = 0; row < N; ++row )\n",
        "\n",
        "    for( int col = 0; col < N; ++col ) {\n",
        "\n",
        "      val = 0;\n",
        "\n",
        "      for ( int k = 0; k < N; ++k ){\n",
        "        val += a[row * N + k] * b[k * N + col];\n",
        "      }\n",
        "\n",
        "      c[row * N + col] = val;\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int *a, *b, *c_cpu, *c_gpu;\n",
        "  int size = N * N * sizeof (int); // Number of bytes of an N x N matrix\n",
        "\n",
        "  // 分配内存\n",
        "  cudaMallocManaged (&a, size);\n",
        "  cudaMallocManaged (&b, size);\n",
        "  cudaMallocManaged (&c_cpu, size);\n",
        "  cudaMallocManaged (&c_gpu, size);\n",
        "\n",
        "  // 初始化数组\n",
        "  for( int row = 0; row < N; ++row )\n",
        "    for( int col = 0; col < N; ++col )\n",
        "    {\n",
        "      a[row * N + col] = row;\n",
        "      b[row * N + col] = col + 2;\n",
        "      c_cpu[row * N + col] = 0;\n",
        "      c_gpu[row * N + col] = 0;\n",
        "    }\n",
        "\n",
        "  dim3 threads_per_block (16, 16, 1); // 一个 16 * 16 的线程阵\n",
        "\n",
        "  dim3 number_of_blocks ((N / threads_per_block.x) + 1, (N / threads_per_block.y) + 1, 1);\n",
        "\n",
        "  matrixMulGPU <<< number_of_blocks, threads_per_block >>> ( a, b, c_gpu ); // 执行核函数\n",
        "\n",
        "  cudaDeviceSynchronize(); // 同步\n",
        "\n",
        "  matrixMulCPU( a, b, c_cpu ); // 执行 CPU 版本的矩阵乘法\n",
        "\n",
        "  // 比较 CPU 和 GPU 两种方法的计算结果是否一致\n",
        "  bool error = false;\n",
        "  for( int row = 0; row < N && !error; ++row ){\n",
        "\n",
        "    for( int col = 0; col < N && !error; ++col ){\n",
        "      if (c_cpu[row * N + col] != c_gpu[row * N + col]) {\n",
        "        printf(\"FOUND ERROR at c[%d][%d]\\n\", row, col);\n",
        "        error = true;\n",
        "        break;\n",
        "      }\n",
        "    }\n",
        "\n",
        "  }\n",
        "\n",
        "  if (!error)\n",
        "    printf(\"Success!\\n\");\n",
        "\n",
        "  // 释放内存\n",
        "  cudaFree(a); cudaFree(b);\n",
        "  cudaFree( c_cpu ); cudaFree( c_gpu );\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyEIyUpYNyX0",
        "outputId": "c6dbc28c-1410-4a2f-e593-d3ba7d8a2e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hello11.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc  hello11.cu -o hello11"
      ],
      "metadata": {
        "id": "c4Htiu73QbDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PvghIbWQdSI",
        "outputId": "1c6454f8-3410-4107-d703-2bae04ff5f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile  hello12.cu\n",
        "\n",
        "#include <math.h>\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "#include <device_launch_parameters.h>\n",
        "\n",
        "#define CHECK(call)                                                     \\\n",
        "do {                                                                    \\\n",
        "    const cudaError_t error_code = call;                                \\\n",
        "    if (error_code != cudaSuccess)                                      \\\n",
        "    {                                                                   \\\n",
        "        printf(\"CUDA ERROR: \\n\");                                       \\\n",
        "        printf(\"    FILE: %s\\n\", __FILE__);                             \\\n",
        "        printf(\"    LINE: %d\\n\", __LINE__);                             \\\n",
        "        printf(\"    ERROR CODE: %d\\n\", error_code);                     \\\n",
        "        printf(\"    ERROR TEXT: %s\\n\", cudaGetErrorString(error_code)); \\\n",
        "        exit(1);                                                        \\\n",
        "    }                                                                   \\\n",
        "}while(0);                                                              \\\n",
        "\n",
        "\n",
        "const double EPSILON = 1.0e-10;\n",
        "const double a = 1.23;\n",
        "const double b = 2.34;\n",
        "const double c = 3.57;\n",
        "\n",
        "\n",
        "// 核函数。\n",
        "__global__ void add(const double *x, const double *y, double *z, const int N);\n",
        "\n",
        "// 重载设备函数。\n",
        "__device__ double add_in_device(const double x, const double y);\n",
        "__device__ void add_in_device(const double x, const double y, double &z);\n",
        "\n",
        "// 主机函数。\n",
        "void check(const double *z, const int N);\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int N = 1e4;\n",
        "    const int M = sizeof(double) * N;\n",
        "\n",
        "    // 申请主机内存。\n",
        "    double *h_x = new double[N];\n",
        "    double *h_y = (double*) malloc(M);\n",
        "    double *h_z = (double*) malloc(M);\n",
        "\n",
        "    // 初始化主机数据。\n",
        "    for (int i = 0; i < N; ++i)\n",
        "    {\n",
        "        h_x[i] = a;\n",
        "        h_y[i] = b;\n",
        "    }\n",
        "\n",
        "    // 申请设备内存。\n",
        "    double *d_x, *d_y, *d_z;\n",
        "    CHECK(cudaMalloc((void**)&d_x, M));\n",
        "    CHECK(cudaMalloc((void**)&d_y, M));\n",
        "    CHECK(cudaMalloc((void**)&d_z, M));\n",
        "\n",
        "    // 从主机复制数据到设备。\n",
        "    CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));\n",
        "    CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));\n",
        "\n",
        "    // 在设备中执行计算。\n",
        "    const int block_size = 128;\n",
        "    const int grid_size = N/128 + 1;\n",
        "    add<<<grid_size, block_size>>>(d_x, d_y, d_z, N);\n",
        "    CHECK(cudaGetLastError());  // 捕捉同步前的最后一个错误。\n",
        "    CHECK(cudaDeviceSynchronize());  // 同步以捕获核函数错误。\n",
        "\n",
        "    // 从设备复制数据到主机。\n",
        "    CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));\n",
        "    check(h_z, N);\n",
        "\n",
        "    // 释放主机内存。\n",
        "    if (h_x) delete[] h_x;\n",
        "    free(h_y);\n",
        "    free(h_z);\n",
        "\n",
        "    // 释放设备内存。\n",
        "    CHECK(cudaFree(d_x));\n",
        "    CHECK(cudaFree(d_y));\n",
        "    CHECK(cudaFree(d_z));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void add(const double *x, const double *y, double *z, const int N)\n",
        "{\n",
        "    // 在主机函数中需要依次对每个元素进行操作，需要使用一个循环。\n",
        "    // 在设备函数中，因为采用“单指令-多线程”方式，所以可以去掉循环、只要将数组元素索引和线程索引一一对应即可。\n",
        "\n",
        "    const int n = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (n > N) return;\n",
        "\n",
        "    if (n%5 == 0)\n",
        "    {\n",
        "        z[n] = add_in_device(x[n], y[n]);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        add_in_device(x[n], y[n], z[n]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__device__ double add_in_device(const double x, const double y)\n",
        "{\n",
        "    return x + y;\n",
        "}\n",
        "\n",
        "__device__ void add_in_device(const double x, const double y, double &z)\n",
        "{\n",
        "    z = x + y;\n",
        "}\n",
        "\n",
        "void check(const double *z, const int N)\n",
        "{\n",
        "    bool has_error = false;\n",
        "    for (int i = 0; i < N ;++i)\n",
        "    {\n",
        "        if (fabs(z[i] - c) > EPSILON)\n",
        "        {\n",
        "            has_error = true;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    printf(\"cuda; %s\\n\", has_error ? \"has error\" : \"no error\");\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-iV57mbUXQ1",
        "outputId": "d74895bf-64dd-4d67-fa68-8300b1bee0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello12.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc  hello12.cu -o hello12"
      ],
      "metadata": {
        "id": "kb7R1QRIUsuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8syfejc1UvJL",
        "outputId": "19f9bf22-4024-4e77-aa4d-04d5a1255cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda; no error\n"
          ]
        }
      ]
    }
  ]
}
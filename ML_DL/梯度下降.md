```markdown
# 线性回归最小二乘法（OLS）参数推导

## 1. 问题定义
给定数据集 \( (x_i, y_i) \)，其中 \( i = 1, 2, \dots, n \)，需找到线性方程 \( y = w_1 x + w_0 \)，使得预测值与真实值的平方误差最小。目标是最小化损失函数：
\[
J(w_0, w_1) = \sum_{i=1}^n \left( y_i - (w_1 x_i + w_0) \right)^2
\]

---

## 2. 求解步骤

### 2.1 对 \( w_0 \) 求偏导并置零
损失函数对截距项 \( w_0 \) 求偏导：
\[
\frac{\partial J}{\partial w_0} = -2 \sum_{i=1}^n (y_i - w_1 x_i - w_0) = 0
\]
整理得到：
\[
\sum_{i=1}^n y_i = w_1 \sum_{i=1}^n x_i + n w_0 \quad \text{(方程1)}
\]

### 2.2 对 \( w_1 \) 求偏导并置零
损失函数对斜率 \( w_1 \) 求偏导：
\[
\frac{\partial J}{\partial w_1} = -2 \sum_{i=1}^n x_i (y_i - w_1 x_i - w_0) = 0
\]
整理得到：
\[
\sum_{i=1}^n x_i y_i = w_1 \sum_{i=1}^n x_i^2 + w_0 \sum_{i=1}^n x_i \quad \text{(方程2)}
\]

---

## 3. 联立方程求解参数

### 3.1 解出截距项 \( w_0 \)
从方程1中解出 \( w_0 \)：
\[
w_0 = \frac{\sum_{i=1}^n y_i - w_1 \sum_{i=1}^n x_i}{n} = \bar{y} - w_1 \bar{x}
\]
其中：
- \( \bar{x} = \frac{1}{n} \sum x_i \) 为 \( x \) 的均值
- \( \bar{y} = \frac{1}{n} \sum y_i \) 为 \( y \) 的均值

### 3.2 解出斜率 \( w_1 \)
将 \( w_0 = \bar{y} - w_1 \bar{x} \) 代入方程2：
\[
\sum x_i y_i = w_1 \sum x_i^2 + (\bar{y} - w_1 \bar{x}) \sum x_i
\]
展开并整理：
\[
\sum x_i y_i = w_1 \left( \sum x_i^2 - \bar{x} \sum x_i \right) + \bar{y} \sum x_i
\]
移项后得：
\[
w_1 = \frac{\sum x_i y_i - \bar{y} \sum x_i}{\sum x_i^2 - \bar{x} \sum x_i}
\]
进一步化简为最终形式：
\[
w_1 = \frac{n \sum x_i y_i - \sum x_i \sum y_i}{n \sum x_i^2 - \left( \sum x_i \right)^2}
\]

---

## 4. 最终公式总结

| 参数  | 公式                                                                 |
|-------|---------------------------------------------------------------------|
| 斜率  | \( w_1 = \frac{n \sum xy - \sum x \sum y}{n \sum x^2 - (\sum x)^2} \) |
| 截距  | \( w_0 = \bar{y} - w_1 \bar{x} \)                                     |

---

## 5. 公式等价性验证
原始截距公式可展开为：
\[
w_0 = \frac{\sum x^2 \sum y - \sum x \sum xy}{n \sum x^2 - (\sum x)^2}
\]
与简化形式 \( w_0 = \bar{y} - w_1 \bar{x} \) 等价（代入 \( w_1 \) 后化简可得）。

---

## 6. 关键结论
1. **斜率 \( w_1 \)** 反映自变量 \( x \) 对因变量 \( y \) 的线性影响强度。
2. **截距 \( w_0 \)** 是回归直线在 \( x=0 \) 处的预测值。
3. 公式为最小二乘法的闭式解，保证全局最优。
```